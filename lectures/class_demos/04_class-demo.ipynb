{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d5691b-ddff-433b-bb28-e7e72d04a7ba",
   "metadata": {},
   "source": [
    "# Lecture 4: Class demo\n",
    "\n",
    "## Image classification using KNNs and SVM RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee761522-b3b6-4a58-9611-6547832e5fb9",
   "metadata": {},
   "source": [
    "For this demonstration I'm using a subset of [Kaggle's Animal Faces dataset](https://www.kaggle.com/datasets/andrewmvd/animal-faces). I've put this subset in our [course GitHub repository](https://github.com/UBC-CS/cpsc330-2023W1/tree/main/lectures/data/animal_faces). \n",
    "\n",
    "The code in this notebook is a bit complicated and you are not expected to understand all the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a7b18-801c-419a-afbe-e53d19ee034c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a521113-8a1a-4f0d-93e8-7e97d08a9358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../code/.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from plotting_functions import *\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef16b0a-2ae3-4959-b1dc-a8786b74470b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, models, transforms, utils\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780ecf87-3a70-4d47-99f6-03aa513846f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m      5\u001b[0m     random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m   \n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_seed\u001b[39m(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m      4\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m      5\u001b[0m     random\u001b[38;5;241m.\u001b[39mseed(seed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "set_seed(seed=42)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75885f10-5f32-4062-9e76-0d6c97197a32",
   "metadata": {},
   "source": [
    "Let's proceed with reading the data. Since we don't have tabular data, we are using a slightly more complex method to read it. You don't need to understand the code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de1de9d-742b-4cd3-b57d-e23c6f593c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "IMAGE_SIZE = 200\n",
    "def read_img_dataset(data_dir):     \n",
    "    \"\"\"\n",
    "    Reads and preprocesses an image dataset from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory path where the dataset is located.\n",
    "\n",
    "    Returns:\n",
    "        inputs (Tensor): A batch of preprocessed input images.\n",
    "        classes (Tensor): The corresponding class labels for the input images.\n",
    "    \"\"\"    \n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),     \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),            \n",
    "        ])\n",
    "               \n",
    "    image_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "         image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0 \n",
    "    )\n",
    "    dataset_size = len(image_dataset)\n",
    "    class_names = image_dataset.classes\n",
    "    inputs, classes = next(iter(dataloader))\n",
    "    return inputs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb25ca9d-4d58-4952-bc18-af2a24e089f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_imgs(inputs):\n",
    "    plt.figure(figsize=(10, 70)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57336b31-dfa6-4908-8dc0-51f62e9da92c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m n_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_names)\n\u001b[1;32m      4\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m n_images  \u001b[38;5;66;03m# because our dataset is quite small\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_anim_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mread_img_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m n_images\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mread_img_dataset\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_img_dataset\u001b[39m(data_dir):     \n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Reads and preprocesses an image dataset from the specified directory.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        classes (Tensor): The corresponding class labels for the input images.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m    \n\u001b[0;32m---> 14\u001b[0m     data_transforms \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m     15\u001b[0m         [\n\u001b[1;32m     16\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mResize((IMAGE_SIZE, IMAGE_SIZE)),     \n\u001b[1;32m     17\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     18\u001b[0m             transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m], [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]),            \n\u001b[1;32m     19\u001b[0m         ])\n\u001b[1;32m     21\u001b[0m     image_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39mdata_dir, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[1;32m     22\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     23\u001b[0m          image_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     24\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "train_dir = \"../data/animal_faces/train\" # training data\n",
    "file_names = [image_file for image_file in glob.glob(train_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "X_anim_train, y_train = read_img_dataset(train_dir)\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd3d65-efd8-45a7-9984-23553dce5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dir = \"../data/animal_faces/valid\" # valid data\n",
    "file_names = [image_file for image_file in glob.glob(valid_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "X_anim_valid, y_valid = read_img_dataset(valid_dir)\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d349a1-400f-4405-953a-5bb15cd2c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_anim_train.numpy()\n",
    "X_valid = X_anim_valid.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e7f9e-a99f-4c79-a178-1e8fb99b9a98",
   "metadata": {},
   "source": [
    "Let's examine some of the sample images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b14f2-fe64-428a-9b56-ce7a8d98afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_imgs(X_anim_train[0:24,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c674f8-f496-48b6-acd6-13292653b523",
   "metadata": {},
   "source": [
    "With K-nearest neighbours (KNN), we will attempt to classify an animal face into one of three categories: cat, dog, or wild animal. The idea is that when presented with a new animal face image, we want the model to assign it to one of these three classes based on its similarity to other images within each of these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a9b52-498a-4742-812a-47bb3a795bdb",
   "metadata": {},
   "source": [
    "To train a KNN model, we require tabular data. How can we transform image data, which includes height and width information, into tabular data with meaningful numerical values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca6eaa-dc4b-40c3-abf7-6ef9f48a9225",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb988d-ca9a-4418-bffb-0822fed2ae52",
   "metadata": {},
   "source": [
    "Flattening images and feeding them to K-nearest neighbors (KNN) is one approach. However, in this demonstration, we will explore an alternative method. We will employ a pre-trained image classification model known as 'desenet' to obtain a 1024-dimensional meaningful representation of each image. The function provided below accomplishes this task for us. Once again, you are not required to comprehend the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ede40e-a951-4906-8576-8002a73c302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model, inputs):\n",
    "    \"\"\"\n",
    "    Extracts features from a pre-trained DenseNet model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A pre-trained DenseNet model.\n",
    "        inputs (torch.Tensor): Input data for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted features from the model.\n",
    "    \"\"\"   \n",
    "    \n",
    "    with torch.no_grad():  # turn off computational graph stuff\n",
    "        Z_train = torch.empty((0, 1024))  # Initialize empty tensors\n",
    "        y_train = torch.empty((0))\n",
    "        Z_train = torch.cat((Z_train, model(inputs)), dim=0)\n",
    "    return Z_train.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaa57c-e733-4fe5-9026-544dc7ebbdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
    "densenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef9b8b-cf06-4c07-91d0-f952877da01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba22ae7-c23c-4816-88d3-d81924064975",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train.reshape(150, 120000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0aec4-4120-4354-9055-95b9db384011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get representations of the train images\n",
    "Z_train = get_features(\n",
    "    densenet, X_anim_train, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f81287-0b93-4aa1-836b-8da3e6642a3d",
   "metadata": {},
   "source": [
    "We now have tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70856c-05c9-43e1-94f9-b32298b8f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366a0e2-c96d-4db2-b2d2-d941c3d46ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ba7b0-7719-4cbd-9dbf-a41b74133212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get representations of the validation images\n",
    "Z_valid = get_features(\n",
    "    densenet, X_anim_valid, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db204d02-4465-4d8e-9c9d-8af481ea9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af9494-5a8e-4ced-935f-0cae87dfc886",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03a781-2244-4dfb-acff-8ef0759fad94",
   "metadata": {},
   "source": [
    "### Dummy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7834e6-57c0-440a-9991-1fc893c234f2",
   "metadata": {},
   "source": [
    "Let's examine the baseline accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73679442-d8ec-4847-8ae9-3ec5db47fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier()\n",
    "pd.DataFrame(cross_validate(dummy, Z_train, y_train, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff9b83-110c-4f22-bbc7-15023852f0c4",
   "metadata": {},
   "source": [
    "## Classification with `KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10650180-5efb-4084-b41a-b6d9f89b684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "pd.DataFrame(cross_validate(knn, Z_train, y_train, return_train_score=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb029bd-c000-4b69-835a-869d44582c12",
   "metadata": {},
   "source": [
    "This is with the default `n_neighbors`. Let's optimize `n_neighbors`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f68223-e0fd-4e79-a3fb-973de6f22b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.get_params()['n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52c856-7fd8-4143-945a-778b62933898",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = np.arange(1, 15, 1).tolist()\n",
    "\n",
    "results_dict = {\n",
    "    \"n_neighbors\": [],\n",
    "    \"mean_train_score\": [],\n",
    "    \"mean_cv_score\": [],\n",
    "    \"std_cv_score\": [],\n",
    "    \"std_train_score\": [],\n",
    "}\n",
    "\n",
    "for k in n_neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_validate(knn, Z_train, y_train, return_train_score=True)\n",
    "    results_dict[\"n_neighbors\"].append(k)\n",
    "\n",
    "    results_dict[\"mean_cv_score\"].append(np.mean(scores[\"test_score\"]))\n",
    "    results_dict[\"mean_train_score\"].append(np.mean(scores[\"train_score\"]))\n",
    "    results_dict[\"std_cv_score\"].append(scores[\"test_score\"].std())\n",
    "    results_dict[\"std_train_score\"].append(scores[\"train_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8409b1-3ae4-4ca6-8673-ae8057dc6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df = results_df.set_index(\"n_neighbors\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa842b-b14c-4e23-af27-fecc5b5a8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[['mean_train_score', 'mean_cv_score']].plot(ylabel='Accuracy', title=\"k vs. accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7023bb-a145-470e-a9d3-26cea34a1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = n_neighbors[np.argmax(results_df['mean_cv_score'])]\n",
    "best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbc606-e4c9-4d2e-9cb1-225a60c7480f",
   "metadata": {},
   "source": [
    "Is SVC performing better than k-NN? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb2b3d-51ab-4998-8c9c-b5b029c0bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = np.logspace(-1, 2, 4)\n",
    "cv_scores = []\n",
    "train_scores = []\n",
    "\n",
    "for C_val in C_values:\n",
    "    print('C = ', C_val)\n",
    "    svc = SVC(C=C_val)\n",
    "    scores = cross_validate(svc, Z_train, y_train, return_train_score=True)\n",
    "    cv_scores.append(scores['test_score'].mean())\n",
    "    train_scores.append(scores['train_score'].mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529bae52-6524-446a-9a00-92fefd812085",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\"cv\": cv_scores, \n",
    "                           \"train\": train_scores},index = C_values)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b53775-3daf-4805-b5b9-5b669985a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = C_values[np.argmax(results_df['cv'])]\n",
    "best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c5dd1-f96a-4f96-84d0-85902b658347",
   "metadata": {},
   "source": [
    "It's not realistic but we are getting perfect CV accuracy with `C=10` and `C=100` on our toy dataset. `sklearn`'s default `C =1.0` didn't give us the best cv score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1646be2-389f-42ac-a073-a29937a81d9e",
   "metadata": {},
   "source": [
    "Let's go back to KNN and manually examine the nearest neighbours. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf7027-2906-4afe-ae77-abe2f0a0c0f7",
   "metadata": {},
   "source": [
    "What are the nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e25ea5-e357-45d6-b0e2-5efa606dfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors()\n",
    "nn.fit(Z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc572f3-58f7-48e2-9b80-04804e931471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not have to understand this code. \n",
    "def show_nearest_neighbors(test_idx, nn, Z, X, y):\n",
    "    distances, neighs = nn.kneighbors([Z[test_idx]])\n",
    "    neighbors = neighs.ravel()\n",
    "    plt.figure(figsize=(2,2), dpi=80)\n",
    "    query_img = X[test_idx].transpose(1, 2, 0)\n",
    "    plt.title('Query image', size=12)\n",
    "    plt.imshow(np.clip(query_img, 0, 255));\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())    \n",
    "    plt.show()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 5, figsize=(10,4), subplot_kw={'xticks':(), 'yticks':()})\n",
    "    print('Nearest neighbours:')\n",
    "    \n",
    "    for ax, dist, img_ind in zip(axes.ravel(), distances.ravel(), neighbors):\n",
    "        img = X_train[img_ind].transpose(1, 2, 0)\n",
    "        ax.imshow(np.clip(img, 0, 255))\n",
    "        ax.set_title('distance: '+ str(round(dist,3)), size=10 )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44555ac-26a2-4b84-b341-910e6f31c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = [1,10,100,140]\n",
    "for idx in test_idx: \n",
    "    show_nearest_neighbors(idx, nn, Z_valid, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0546d-0cce-4e90-a704-ebfc44a9f4da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
